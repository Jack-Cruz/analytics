{"cells":[{"cell_type":"markdown","metadata":{"id":"SYd3bpaYiyYZ"},"source":["# Construyendo una red neuronal Paso a Paso\n","\n","**En esta sesión haremos**:\n","- Haremos una red neuronal con más de 1 capa oculta.\n","- Vamos a implementar una clase que contendrá una red Neuronal.\n","\n","**Notación**:\n","- Super-índide $[l]$ denota una cantidad asociada a la $l^{th}$ capa. \n","    - Ejemplo: $a^{[L]}$ es la $L^{th}$ capa de activación. $W^{[L]}$ y $b^{[L]}$ son los $L^{th}$ parámetros de la capa.\n","- Super-índide $(i)$ denota una cantidad aosciada al $i^{th}$ registro. \n","    - Ejemplo: $x^{(i)}$ es el $i^{th}$ registro de entrenamiento.\n","- Sub-índice $i$ denota la $i^{th}$ entrada de un vector.\n","    - Ejemplo: $a^{[l]}_i$ denota la $i^{th}$ entrada de las $l^{th}$ capas de activación)."]},{"cell_type":"markdown","metadata":{"id":"HZO5dQKliyYd"},"source":["## 1 - Paquetes\n","\n","- [numpy](www.numpy.org) Paquete para computación científica.\n","- [matplotlib](http://matplotlib.org) Paquete para hacer gráficos en Python."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qv8QJEPMiyYe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644418028361,"user_tz":300,"elapsed":313,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}},"outputId":"220ece9f-774b-445d-c15e-08d15de0443b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"K4og0iGOiyYf"},"source":["## 2 - Lo que desarrollaremos\n","\n","Para construir su red neuronal, implementará varias \"funciones auxiliares\". Estas funciones auxiliares se usarán en la próxima tarea para construir una red neuronal de dos capas y una red neuronal de \"L\" capas.\n","\n","Desarrollaremos lo siguiente:\n","\n","- Incializar parámetros para una red de 2 capas y para extrapolar en una red de \"L\" capas.\n","- Se implmentará como es que se realiza el Forward propagation.\n","- Se calculará el costo.\n","- Se implementará como es que se realiza el Backward propagation.\n","- Actualización de parámetros."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"qDXE279MiyYg"},"source":["## 3 - Inicialización\n","\n","Haremos funciones de inicialización de parámetros para 2 capas y luego su extención a \"L\" capas ocultas.\n","\n","### 3.1 - Para una Red Neural de 2 capas\n","\n","- El modelo que haremos tendra una estructura: *LINEAL -> RELU -> LINEAL -> SIGMOIDEA*. \n","- Se usarán pesos aleatorios para las matrices de pesos. \n","- Para le bias se usará \"0\" como incialización."]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"-8JZNwBdiyYh","executionInfo":{"status":"ok","timestamp":1644418028803,"user_tz":300,"elapsed":51,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Entradas:\n","    n_x -- Tamaño de la capa de entrada\n","    n_h -- Tamaño de la capa oculta\n","    n_y -- Tamaño de la capa de salida\n","    \n","    Salidas:\n","    parameters -- Diccionario de python que contiene los parámetros:\n","                    W1 -- Matriz de pesos de tamaño (n_h, n_x)\n","                    b1 -- Vector de Bias de tamaño (n_h, 1)\n","                    W2 -- Matriz de pesos de tamaño (n_y, n_h)\n","                    b2 -- Vector de Bias de tamaño (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    ### Inicializacion de Parametros\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros(shape=(n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros(shape=(n_y, 1))\n","    \n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":10,"metadata":{"scrolled":true,"id":"9WnrQ-JxiyYi","outputId":"e7272046-ffed-49b1-fc28-381405fa9af3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644418028805,"user_tz":300,"elapsed":51,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]]\n","b1 = [[0.]\n"," [0.]]\n","W2 = [[ 0.01744812 -0.00761207]]\n","b2 = [[0.]]\n"]}],"source":["parameters = initialize_parameters(3,2,1)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"-o3ioR0jiyYj"},"source":["### 3.2 - Inicialización para una Red Neuronal de \"L\" capas\n","\n","La inicialización para una red neuronal de \"L\" capas es mas complicado, debido a que hay mas matrices de pesos y vectores de bias.\n","\n","Recorda que cuando se calcula $W X + b$ en python se desarrolla lo siguiente:\n","\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r \n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i \n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","Cuando $WX + b$ este será:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$"]},{"cell_type":"markdown","metadata":{"id":"raEyasOyiyYk"},"source":["Implementación de una red neuronal con \"L\" capas.\n","\n","- La estructura del modelo será *[LINEAL -> RELU] $ \\times$ (L-1) -> LINEAL -> SIGMOIDEA*. Esto quiere decir que tendrá $L-1$ capas usando la funcion de activación tipo ReLU seguidopor una capa con función de activación sigmoidea.\n","- Guardaremos $n^{[l]}$, que es el numero de unidades de diferentes capas, en la variable `layer_dims`."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"QmNejicSiyYk","executionInfo":{"status":"ok","timestamp":1644418028808,"user_tz":300,"elapsed":41,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Entradas:\n","    layer_dims -- Lista que contine las dimensiones de cada capa de nuestra red\n","    \n","    Salidas:\n","    parameters -- Diccionario que contiene los parámetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- Matriz de pesos de tamaño (layer_dims[l], layer_dims[l-1])\n","                    bl -- Vector Bias de tamaño (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims)            # Numero de capas de la red\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"aQcK81NNiyYl","outputId":"cc4bd36e-1f80-44f9-cb5a-37463039040c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644418028810,"user_tz":300,"elapsed":41,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"]}],"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"vqTK6KnaiyYl"},"source":["## 4 - Modulo para Forward propagation\n","\n","### 4.1 - Linear Forward .\n","Ahora que se han inicializado los parámetros, se realizará el módulo de forward propagation, implementando algunas funciones básicas.\n","\n","- LINEAL FORWARD\n","- LINEAL -> ACTIVACION donde la ACTIVACION será ReLU o Sigmoidea. \n","- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDEA (Para todo el modelo)\n","\n","El modulo de linear forward usará las siguientes ecuaciones:\n","\n","$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n","\n","donde $A^{[0]} = X$. \n","\n","Cabe recordar que la representación matemática de esta unidad es $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ePBDB4XviyYm","executionInfo":{"status":"ok","timestamp":1644418028812,"user_tz":300,"elapsed":33,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implementacion de Forward Propagation de una capa.\n","\n","    Entradas:\n","    A -- Activaciones de las capas previas o la data de entrada: (tamaño de la capa previa o del numero de registros)\n","    W -- Matriz de pesos: Arreglo de tamaño (tamaño de la capa actual, Tamaño de la capa anterior)\n","    b -- Vector Bias, Arreglo de tamaño (tamaño de la capa actual, 1)\n","\n","    Salidas:\n","    Z -- La entrada de la funcion de activacion, tambien llamada parámetro de pre-activación \n","    cache -- Diccionario que contiene a \"A\", \"W\" y \"b\" ; guardados para el paso de backward propagation\n","    \"\"\"\n","    \n","    Z = np.dot(W, A) + b\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"]},{"cell_type":"code","source":["from testCases_v2 import * # casos estáticos\n","from dnn_utils_v2 import * # relu, sigmoidea"],"metadata":{"id":"CnKVbAXrYFFI","executionInfo":{"status":"ok","timestamp":1644418029243,"user_tz":300,"elapsed":460,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"psAWzKcFiyYm","outputId":"cb3a2c40-4265-4b5d-e810-80970b272f6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644418035334,"user_tz":300,"elapsed":323,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Z = [[ 3.26295337 -1.23429987]]\n"]}],"source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"]},{"cell_type":"markdown","metadata":{"id":"owFx77V3iyYn"},"source":["### 4.2 - Activación Lineal\n","\n","Se usará 2 funciones de activación:\n","\n","- **Sigmoidea**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Esta función ya se ecnuentra cargada como función `sigmoid`. Esta función devuelve **dos** valores: La activación de valor \"`a`\" y un \"`cache`\" que contiene a \"`Z`\" (que es la que será usada). Para usarla solo se debe hacer: \n","``` python\n","A, activation_cache = sigmoid(Z)\n","```\n","\n","- **ReLU**: La fórmula matemática de ReLU es $A = RELU(Z) = max(0, Z)$. Esta función tambien se encuentra carga con el nombre de función `relu`. Esta función devuelve **dos** valores: El valor de activación \"`A`\" y el \"`cache`\" que contiene a \"`Z`\" (que es la que será usada). Para usarlo solo se debe hacer:\n","``` python\n","A, activation_cache = relu(Z)\n","```"]},{"cell_type":"markdown","metadata":{"id":"aqtGqYXUiyYn"},"source":["Por conveniencia se agruparán las dos funciones en una sola funcion (LINEAL->ACTIVACION).\n","\n","Ahora implementaremos el forward propagation de la capa *LINEAL->ACTIVACION* layer, cuya relación matemática es: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ donde la activación \"g\" podrá ser sigmoid() o relu()."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jDF_SAUgiyYn","executionInfo":{"status":"ok","timestamp":1644418507745,"user_tz":300,"elapsed":334,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Entradas:\n","    A_prev -- Activaciones de la la anterior capa o la data de entrada: (tamaño de la capa anterior, número de registros)\n","    W -- Matriz de pesos: Arreglo de tamaño (tamaño de la capa actual, tamaño de la capa anterior)\n","    b -- Vector Bias, arreglo de tamaño (tamaño de la capa actual, 1)\n","    activation -- el tipo de activación que se usará: \"sigmoid\" o \"relu\"\n","\n","    Salidas:\n","    A -- La salida de la función de activación, también llamado valor de post-activation \n","    cache -- Diccionario que contiene a \"linear_cache\" y \"activation_cache\" para \n","             el paso de backward\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cJEbG4uiyYo","outputId":"0fd63ad7-481e-4afd-a83a-271054bb0bcf","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"]}],"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"]},{"cell_type":"markdown","metadata":{"id":"civkcoTziyYo"},"source":["**Note**: In deep learning, the \"[LINEAR->ACTIVATION]\" computation is counted as a single layer in the neural network, not two layers. "]},{"cell_type":"markdown","metadata":{"id":"7mhVxxNXiyYp"},"source":["### d) Modelo con \"L\" Capas \n","\n","Como ya implementamos las activaciones, ahora usaremos las mismas funciones para usarlas de la siguiente manera: `linear_activation_forward` con RELU $L-1$ veces, a las que le seguira una capa `linear_activation_forward` con SIGMOID.\n","\n","PAra el código, la variable `AL` será $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (Tambien llamada `Yhat`, i.e., este es $\\hat{Y}$.) \n","\n","**Desarrollo**:\n","- Usaremos las funciones previamente escritas.\n","- En un loop usaremos [LINEAR->RELU] (L-1) veces."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"7yN6OkHsiyYp","executionInfo":{"status":"ok","timestamp":1644419236593,"user_tz":300,"elapsed":376,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["# GRADED FUNCTION: L_model_forward\n","\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","\n","    Entradas:\n","    X -- data, de tamaño (numero de variables, número de registros)\n","    parameters -- salida de initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- Ultimo valor de post-activación\n","    caches -- lista de caches que contiene:\n","                 cada caché de linear_relu_forward() (hay L-1 de ellos, indexados desde 0 a L-2)\n","                 el caché de linear_sigmoid_forward() (hay uno, indexado como L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # numero de capas en la red neuronal\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev,\n","                                             parameters['W{}'.format(l)],\n","                                             parameters['b{}'.format(l)],\n","                                             'relu')\n","        caches.append(cache)\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    AL, cache = linear_activation_forward(A,\n","                                          parameters['W{}'.format(L)],\n","                                          parameters['b{}'.format(L)],\n","                                          'sigmoid')\n","    caches.append(cache)\n","    \n","    assert(AL.shape == (1,X.shape[1]))\n","            \n","    return AL, caches"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"uVIk4RBciyYp","outputId":"d0d937ee-eebd-48d3-a548-5320cedf6dcf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644419999992,"user_tz":300,"elapsed":323,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["AL = [[0.17007265 0.2524272 ]]\n","Length of caches list = 2\n"]}],"source":["X, parameters = L_model_forward_test_case()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"]},{"cell_type":"markdown","metadata":{"id":"ZdSlwIRmiyYq"},"source":["Ahora que ya tenemos a el forward propagation desarrolaldo completamente este toma como entrada un input X y salidas vectoriales $A^{[L]}$ que continen las predicciones. Así mismo tenemos valores intermedios en \"chachés\". Entonces usando $A^{[L]}$, se podrá calcular el costo de nuestras predicciones."]},{"cell_type":"markdown","metadata":{"id":"Dn_djss4iyYq"},"source":["## 5 - Función de Costo\n","\n","Ahora se implementará el forward y backward propagation. PAra esto, se necesita calcular el costo, ya que asi podremos saber si el modelo efectivamente esta aprendiendo.\n","\n","**Desarrollo**: Se calculará el costo tipo cross-entropy $J$, usando la siguiente formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"CToM8JYmiyYq","executionInfo":{"status":"ok","timestamp":1644420969383,"user_tz":300,"elapsed":4,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[],"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Funcion de calculo de costo.\n","\n","    Entradas:\n","    AL -- Vector de probabilidades correspondiente la mis predicciones reales  (1, numero de registros)\n","    Y --  Vector de \"Etiquetas\" verdaderas (Pro ejemplo: 0 si no es un gato, 1 si es un gato) (1, numero de registros)\n","\n","    Salidas:\n","    costo -- costo tipo cross-entropy\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"c0_tQ96viyYq","outputId":"4ab25a77-3575-4168-b6b0-a80d41264e1d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644420969384,"user_tz":300,"elapsed":5,"user":{"displayName":"Jack Yefri Cruz Mamani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgR-Tffb6J_CECUfTNZAo0vhdq62A2y6wnzb94K=s64","userId":"06470860857688434831"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["cost = 0.41493159961539694\n"]}],"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"]},{"cell_type":"markdown","metadata":{"id":"TLas7RcliyYr"},"source":["## 6 - Modulo de Backpropagation\n","\n","Tal cual el modulo de forward propagation, se implementaran las funciones necesarias para el back propagation. Recordar que el backpropagation es usado para calcular la gradiente de la funcion de costo con sus respectivos parámetros. \n","\n","Ahora, similar al forward propagation, se harán los siguientes pasos:\n","- LINEAR backward\n","- LINEAR -> ACTIVATION backward donde ACTIVATION calcula la derivativa de o la funcion de activación Sigmoid o ReLU.\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward"]},{"cell_type":"markdown","metadata":{"id":"Mja8tLGpiyYr"},"source":["### 6.1 - Linear backward\n","\n","Para $l$ capas, la parte lineal será: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguido por su activación).\n","\n","Supongamos que ya se tiene calculado la derivada de  $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Lo que queremos obtender será $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n","\n","Las tres salidas serán $(dW^{[l]}, db^{[l]}, dA^{[l]})$ que son calculados usando la entrada $dZ^{[l]}$. Estas será las formulas que usaremos:\n","$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"S_7kpT3MiyYr"},"outputs":[],"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    Se implementará la porción lineal del backward porpagation para una sola capa (capa l)\n","\n","    Entradas:\n","    dZ -- Gradiente del costo respecto a la salida lineal (capa actual l)\n","    cache -- Tupla de valores (A_prev, W, b) tomados desde el forward propagation en la capa actual\n","\n","    Salidas:\n","    dA_prev -- Gradiente del costo con respecto a su activación (de la capa previa l-1), con mismo tamaño de A_prev\n","    dW -- Gradiente del costo con respecto a W (capa actual l), con el mismo tamaño de W\n","    db -- Gradient del costo con respecto a b (capa actual l), con el mismo tamaño de b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","\n","    dW = 1 / m * np.dot(dZ, A_prev.T)\n","    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","     \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL-AKoBUiyYs","outputId":"b613a5de-b780-4ac7-e679-6470ca1c83cc","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev = [[ 0.51822968 -0.19517421]\n"," [-0.40506361  0.15255393]\n"," [ 2.37496825 -0.89445391]]\n","dW = [[-0.10076895  1.40685096  1.64992505]]\n","db = [[0.50629448]]\n"]}],"source":["# Set up some test inputs\n","dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"markdown","metadata":{"id":"avvIXrnUiyYs"},"source":["### 6.2 - Linear-Activation backward\n","\n","Ahora, crearemos la función que une dos funciones auxiliares:\n","**`linear_backward`** y el paso de activación backward \n","**`linear_activation_backward`**. \n","\n","Se implementará `linear_activation_backward`, usando las funciones dadas:\n","- **`sigmoid_backward`**: Implementa el backward propagation para unidades sigmoides. Uso:\n","\n","```python\n","dZ = sigmoid_backward(dA, activation_cache)\n","```\n","\n","- **`relu_backward`**: Implementa el backward propagation para unidades ReLU. Uso:\n","\n","```python\n","dZ = relu_backward(dA, activation_cache)\n","```\n","\n","Si $g(.)$  es la función de activación, \n","`sigmoid_backward` y `relu_backward` calculan  $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lyHeO81iiyYs"},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Se implementará la capa LINEAR->ACTIVATION.\n","    \n","    Entradas:\n","    dA -- Gradiente de post-activación para la capa actual layer l \n","    cache -- Tupla de valores (linear_cache, activation_cache) para el cálculo de backward propagation\n","    activation -- Es la activación usada para esta capa: \"sigmoid\" o \"relu\"\n","    \n","    Salidas:\n","    dA_prev -- Gradiente del costo con respecto a la activación (de la capa anterior l-1), con el mismo tamañao A_prev\n","    dW -- Gradiente del costo con respecto a W (de la misma capa l), con el mismo tamaño que W\n","    db -- Gradiente del costo con respecto a b (de la misma capa l), con el mismo tamaño que b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","        \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","    \n","    # Shorten the code\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SF8eXoFViyYs","outputId":"efa3939e-dfe1-4480-e4b4-f04d52540561","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"]}],"source":["AL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"]},{"cell_type":"markdown","metadata":{"id":"MxMhexafiyYt"},"source":["### 6.3 - Backward para un modelo de \"L\" capas \n","\n","Ahora se implementará el total de la red neuronal. Ya se implementó la funcion `L_model_forward`, donde en cada iteración, se almacena el caché que contiene (X,W,b, y z). En el módulo de backpropagation, se usarán esas variables para calcular las gradientes. Entonces, en la función `L_model_backward`, se iterarán todas las capas ocultas, empezando desde la capa $L$. En cada paso, se usarán los valores caché de la capa $l$ para \"backpropagar\" a la capa $l$.\n","\n","** Inicio**:\n","Para hacer backpropagation, se sabe que la salida es\n","$A^{[L]} = \\sigma(Z^{[L]})$. Entonces el codigo que necesitamos colocar es `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n","Se usará esta fórmula (resultado de la derivada usando cálculo matemático):\n","```python\n","dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n","```\n","\n","Ahora, podremos usar la gradiente de post-activación `dAL` para continuar con el backward, entonces podremos usar `dAL` en la funcion implementada LINEAR->SIGMOID backward (L_model_forward function). Luego de esto, usaremos un loop para iterar cada una de las capas usando la funcion LINEAR->RELU backward.\n","Se usarán todos los datos guardados dA, dW, y db de el diccionario de gradientes. Entonces usaremos la fórmula:\n","\n","$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n","\n","Por ejemplo, para $l=3$ este guardará $dW^{[l]}$ en `grads[\"dW3\"]`.\n","\n","Implementaremos backpropagation para *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Ss43RvuhiyYt"},"outputs":[],"source":["def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Entradas:\n","    AL -- Vector de probabilidades, salida del forward propagation (L_model_forward())\n","    Y -- Vector de \"etiquetas\" verdaderas\n","    caches -- lista de chaches que contienen:\n","                cada caché de linear_activation_forward() con \"relu\" (sus caches[l], desde l en range(L-1) i.e l = 0...L-2)\n","                el caché de linear_activation_forward() con \"sigmoid\" (sus caches[L-1])\n","    \n","    Salidas:\n","    grads -- Diccionario de gradientes\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # numero de capas\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # Lueog de esta linea, Y es del mismo tamañao que AL\n","    \n","    # Inicializando el backpropagation\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradientes. Entradas: \"AL, Y, caches\". Salidas: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n","    current_cache = caches[-1]\n","    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,\n","                                                                                                  current_cache,\n","                                                                                                  \"sigmoid\")\n","    \n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradientes.\n","        # Entradas: \"grads[\"dA\" + str(l + 2)], caches\". Salidas: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA{}\".format(l + 2)],\n","                                                                    current_cache,\n","                                                                    \"relu\")\n","        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-Rmt7fQiyYu","outputId":"a32cc2ac-c078-4d1f-85b6-b24507265f50","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n"," [0.         0.         0.         0.        ]\n"," [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n","db1 = [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]]\n","dA1 = [[ 0.          0.52257901]\n"," [ 0.         -0.3269206 ]\n"," [ 0.         -0.32070404]\n"," [ 0.         -0.74079187]]\n"]}],"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print (\"dW1 = \"+ str(grads[\"dW1\"]))\n","print (\"db1 = \"+ str(grads[\"db1\"]))\n","print (\"dA1 = \"+ str(grads[\"dA1\"]))"]},{"cell_type":"markdown","metadata":{"id":"meHtLC5siyYu"},"source":["### 6.4 - Actualización de Parámetros\n","\n","Usaremos la gradiente descendente para actualizar los parámetros\n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n","\n","donde $\\alpha$ es el factor de aprendizaje. Luego de calcular los parámetros actualizados, se guardan dentro del diccionario de parámetros."]},{"cell_type":"markdown","metadata":{"id":"xEFJ7Xb4iyYu"},"source":["Se implementará `update_parameters()` para actualizar los parámetros usando gradiente descendente.\n","\n","Parámetros a actualizar: $W^{[l]}$ y $b^{[l]}$ desde $l = 1, 2, ..., L$. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"F7Pk4j3QiyYu"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Actualizar paramtros usando gradiente descendente\n","    \n","    Entradas:\n","    parameters -- diccionario que contiene parámetros\n","    grads -- diccionario que contiene gradientes, salida de L_model_backward\n","    \n","    Salidas:\n","    parameters -- diccionario que contiene los parametros actualizados \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # numero de capas en la red neuronal\n","\n","    # Regla de actualización para cada parámetros.\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lOALY98iyYv","outputId":"49afdf20-6371-492b-b166-a822f7f61f9c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}],"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ao4W3UlgiyYv"},"outputs":[],"source":[""]}],"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Construyendo_una_red_neuronal_Paso_a_Paso.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}